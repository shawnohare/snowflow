import json
from pathlib import Path
import logging

import boto3

logger = logging.getLogger(__name__)


def load_metadata(bucket: str, prefix: str) -> dict[str, dict]:
    """
    Get general export metadata file and table status metadatas. The
    former contains column names / types as well as a string containig
    schema and table name.

    Export metadata contains info about the overall RDS snapshot export. For
    example
        {
            'exportTaskIdentifier': 'v2-r1000-20210307-snapshot',
            'sourceArn': 'arn:aws:rds:us-west-2:025524405457:cluster-snapshot:rds:v2-production-r1000-cluster-2021-03-07-08-50',
            'exportOnly': [],
            'snapshotTime': 'Mar 7, 2021 8:50:42 AM',
            'taskStartTime': 'Mar 8, 2021 5:32:17 AM',
            'taskEndTime': 'Mar 8, 2021 8:04:43 AM',
            's3Bucket': 'skupos-partner-snowflake-121345678',
            's3Prefix': 'v2-r1000-',
            'exportedFilesPath': 'v2-r1000-/v2-r1000-20210307-snapshot',
            'iamRoleArn': 'arn:aws:iam::025524405457:role/service-role/skuposRDS-export-s3-role',
            'kmsKeyId': 'arn:aws:kms:us-west-2:025524405457:key/c86308ac-ca14-40b0-af6f-832e507461db',
            'status': 'COMPLETE',
            'percentProgress': 100,
            'totalExportedDataInGB': 28057.983845219016,
        }

    Tables metadata is a list of dicts containing table metadata for tables in
    the input schema. Each table metadata object for a successfully exported
    table looks like
        {
            'tableStatistics': {
                'extractionStartTime': 'Mar 8, 2021 7:35:14 AM',
                'extractionEndTime': 'Mar 8, 2021 7:35:42 AM',
                'partitioningInfo': {
                    'numberOfPartitions': 1,
                    'numberOfCompletedPartitions': 1,
                },
            },
            'schemaMetadata': {
                'originalTypeMappings': [
                    {
                        'columnName': 'stranscd',
                        'originalType': 'bigint',
                        'expectedExportedType': 'int64',
                        'originalCharMaxLength': 0,
                        'originalNumPrecision': 19,
                        'originalDateTimePrecision': 0,
                    },
                    {
                        'columnName': 'descrpt',
                        'originalType': 'text',
                        'expectedExportedType': 'binary (UTF8)',
                        'originalCharMaxLength': 65535,
                        'originalNumPrecision': 0,
                        'originalCharsetName': 'latin1',
                        'originalDateTimePrecision': 0,
                    },
                ],
            },
            'status': 'COMPLETE',
            'sizeGB': 8.046627044677734e-07,
            'target': 'tdlinx.tdlinx.stranscd',
            # inferred keys
            'schema': 'tdlinx',
            'table': 'stranscd',
        }

    A skipped export still contains a 'target' key but might include
    a 'warningMessage'.

    There does not appear to be any retention of indices.

    The type mappings are used to create tables.

    We transform the metadata stored in s3 by converting some lists to
    (order-preserving) dicts and renaming the column metadata fields.
    The original metadata files generated by AWS are enveloped and cached at
    `~/.cache/snowflow/{prefix}/metadata.json`

    Args:
        bucket: Bucket in which the RDS export is located. This should
            be accessible by the Snowflake AWS Integration.
        prefix: Export prefix pointing to the actual content dir containing
            export metadata and data. For example, "snapshots/{rds_instance}/{date}"

    Returns:
        A dict containing general export metadata under the 'info' key
            and table metadata under the 'tables_info' key.
            The dict keyed by 'tables_info' contains a single key containg a list
            of dicts. Each element contains metadata about a specific
            table, namely the column names, datatypes, export status, and
            "target". The "target" value typically looks like
            "{{schema}}.{{schema}.{{table}}", and corresponds to
            an s3 path fragment "{{schema}}/{{schema}}.{{table}}". Sometimes
    """
    metapath = Path("~/.cache/snowflow", prefix, "metadata.json").expanduser()
    metapath.parent.mkdir(parents=True, exist_ok=True)

    # Grab the two export_*.json metadata files, combine into a single json
    # file and cache the output.
    if not metapath.exists():
        logger.info(f"Pulling export metadata from s3://{bucket}/{prefix}.")
        prefix = str(Path(prefix, "export_"))
        summaries = boto3.resource("s3").Bucket(bucket).objects.filter(Prefix=prefix)

        # Look for the "export_tables_info*" and "export_info*" json files and
        # load them into a buffer.
        buf = {}
        for summary in summaries:
            okey = summary.key
            if "tables_info" in okey:
                key = "tables"
            elif "export_info" in okey:
                key = "export"
            else:
                continue
            obj = summary.get()
            data = json.loads(obj["Body"].read().decode())
            buf[key] = data
        with metapath.open("w") as mfile:
            logger.info(f"Caching metadata in {metapath}.")
            json.dump(buf, mfile)
    else:
        logger.info(f"Loading cached export metadata from {metapath}.")
        with metapath.open() as mfile:
            buf = json.load(mfile)

    if 'export' not in buf and 'tables' not in buf:
        raise ValueError(f'No metadata found for RDS export at s3://{bucket}/{prefix}')

    # Embed table status in main metadata, remove some extraneous layering.
    # Convert table lists to lookups. Order is preserved.
    meta = buf["export"]
    schemas = meta.setdefault("schemas", {})

    # Group the table metadata by schema and add some default objects.
    for table in buf["tables"]["perTableStatus"]:
        # Parse the "target" key for each table's metadata into "schema"
        # and "table".
        target = table["target"]
        try:
            _, schema, name = target.split(".")
        except ValueError:
            schema = target
            name = target
        table["schema"] = schema
        table["name"] = name
        # Some tables are skipped, e.g., if the db is empty, so they won't have
        # type mappings. Add empty objects here.
        table["columns"] = []

        # Rename fields for each column metadata and convert list to dict.
        for column in table.get("schemaMetadata", {}).pop("originalTypeMappings", []):
            tcolumn = {
                "name": column.pop("columnName", None),
                "original_type": column.pop("originalType", None),
                "exported_type": column.pop("expectedExportedType", None),
                "char_max_length": column.pop("originalCharMaxLength", None),
                "num_precision": column.pop("originalNumPrecision", None),
                "datetime_precision": column.pop("originalDateTimePrecision", None),
                "charset": column.pop("originalCharsetName", None),
            }
            table["columns"].append(column | tcolumn)

        if schema not in schemas:
            schemas[schema] = {"tables": []}
        schemas[schema]["tables"].append(table)

    return meta
